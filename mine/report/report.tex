% ========================================================
% PROJECT TEMPLATE, *KNOWLEDGE DISCOVERY*, Summer 2018
% University of Potsdam, by Christoph Schommer, University
% of Luxembourg; 
% ========================================================

\documentclass[11pt]{article}
\usepackage{geometry}
\usepackage{enumitem}
\usepackage{graphicx}
\usepackage[usenames,dvipsnames]{color}

%
\newcommand{\code}[1]{\textbf{#1}}
\newcommand{\para}[0]{\par\vspace{0.5cm}}

%
\def\MakeMeBlue#1{\textcolor{Blue}{#1}}
\pagenumbering{arabic}
%

\parindent 0pt

%

\title{\MakeMeBlue{Data reliability for the Household Power Consumption Dataset}}
\author{Miroslav Vitkov}
\date{\today}

% ========================================================
\begin{document}
\maketitle

% ========================================================
\section{Introduction}
The Household Power Consumption database was donated to the public domain by Georges Hebrail in 2010.
It contains 9 attributes, about 1.25\% of the measurements are missing.
\para
The goal of the current analysis is to gleam the following insights out of the database:
\begin{itemize}
    \item{Judge data integrity.}
    \item{Determine situation causing missing data.}
    \item{Determine situation causing incorrect data.}
\end{itemize}
\para
To that end, the data needs to be cleaned, sanity checked and converted to a consistent representation.
The initial state of the data is plain instrument readings.

% ========================================================
\section{Related Work}
Chujai et al\cite{q1} explore household power prediction.
Despite some grammatical errors, they state that the ARIMA algorithm is a good predictor.
\para 
Some Hébrail has explored privacy issues with household data collection\cite{q2}.
% ========================================================
\section{Methodology}
\code{R} with \code{tidyverse} has been used as modern statistical software.
All source code is publicly available for examination and scrutiny.

% ========================================================
\section{Implementation}
\subsection{Getting the data}
The database is of significant size, spanning 2075259 instances.
This is time consuming both for a human and for a CPU to work on.
Therefore, the script \code{get\_dataset.sh} has been written.
It accepts a numeric parameter of how many instances to be downloaded.
If no parameter is passed, the whole dataset is downloaded.

\subsection{Reading the data}
The data reading step includes several subtle transformations on the data.
\para
Firstly, \code{Date} and \code{Time} columns are merged into a single \code{time} attribute.
This immensely aids operations with times, like verification or time differences.
\para
Then the rest of the attributes are converted to standard SI units.
In order to insure physically sound analysis, the measurement units of the attributes are included in their names.
\para
Power factor `pf` and Active energy in the rest of the house \code{active4.W} attributes are injected, as those will be often used.
\para
The whole thing is represented as a \code{tidyverse::tibble} - a modern best practice among R programmers. 

\subsection{Cleaning the data}
There are a couple major challenges in cleaning the data.
\para
One of them are missing values.
I have selected a conservative approach of dropping datapoints with missing values.
This decision has been motivated by several observations:
\begin{itemize}
    \item{The missing values are less than 2\% of the data.}
    \item{When values are missing, everything except \code{Date} and \code{Time} is missing at the same time.}
    \item{Although the new irregular timescale hinders the analysis (and particularly continuous time estimates such as regressions), this approach is believed to introduce the least am mount of subjectivity.}
\end{itemize}
\para
Another major challenge were inconsistent values.
For example, very early in the analysis it was observed that Power factor values routinely exceed their theoretical maximum value of 1.0.
Power factor violations constitute about 52\% of the remaining data after dropping empty measurements.
The following approaches were considered - and rejected - for coping with said problem.
\begin{itemize}
    \item{Drop offending roles - rejected because of high percentage of offending rows and low error band}
    \item{Correct \code{pf} and immediately related quantities - not feasible because of the large subjective skew to be introduced to the database.}
    \item{Clip \code{pf} to valid range - not performed as then the other quantities would have produced ghost deviations and undetectable errors.}
\end{itemize}
Consequently, \code{pf} verification was just turned off.
All other violations were filtered out.


\subsection{Regression}
% ========================================================
\section{Conclusion}
Missing data is both rare and easily detectable.
On the other hand, data inconsistencies are both hard to detect and widespread.
It was shown that more than half of the database constitutes of physically impossible measurements.
\para
Those conclusions put extreme importance on validating datasets with domain experts, before using them for machine learning.

% ========================================================

\begin{thebibliography}{9}

\bibitem{q1}
  Pasapitch Chujai, Nittaya Kerdprasop, and Kittisak Kerdprasop,
  \textit{Time Series Analysis of Household Electric Consumption with ARIMA and ARMA Models},
  Proceedings of the International MultiConference of Engineers and Computer Scientists 2013 Vol I,
  IMECS 2013, March 13 - 15, 2013, Hong Kong

\bibitem{q2}
  Georges Hébrail,
  \textit{P RIVACY - PRESERVING USE OF INDIVIDUAL SMART METERING DATA FOR CUSTOMER SERVICES}

\end{thebibliography}
 
\end{document}
