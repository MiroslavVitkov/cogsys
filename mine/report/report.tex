% ========================================================
% PROJECT TEMPLATE, *KNOWLEDGE DISCOVERY*, Summer 2018
% University of Potsdam, by Christoph Schommer, University
% of Luxembourg; 
% ========================================================

\documentclass[11pt]{article}
\usepackage{geometry}
\usepackage{enumitem}
\usepackage{graphicx}
\usepackage[usenames,dvipsnames]{color}


\newcommand{\code}[1]{\textbf{#1}}
\newcommand{\para}[0]{\par\vspace{0.5cm}}

%
\def\MakeMeBlue#1{\textcolor{Blue}{#1}}
\pagenumbering{arabic}
%

\parindent 0pt

%

\title{\MakeMeBlue{title?}}
\author{Miroslav Vitkov}
\date{\today}

% ========================================================
\begin{document}
\maketitle

% ========================================================
\section{Introduction}
The Household Power Consumption database was donated to the public domain by Georges Hebrail in 2010.
It contains 9 attributes, about 1.25\% of the measurements are missing.
\para
The goal of the current analysis is to gleam the following insights out of the database:
\begin{itemize}
    \item{whatever}
\end{itemize}
\para
To that end, the data needs to be cleaned, sanity checked and converted to a consistent representation.
The initial state of the data is plain instrument readings.

% ========================================================
\section{Related Work}
TODO: just google it


% ========================================================
\section{Methodology}
Introduce the machine learning technique(s) or the algorithm(s) used in the project. \\

An example of inserting figures. Position and width of the figure can be adjusted as needed.
\begin{figure}[!htp]        
  \centering
    \includegraphics[width=0.2\textwidth]{MyProject-KnowledgeDiscovery/image.jpg}
    \caption{Brief description of the figure.}
\end{figure}

% ========================================================
\section{Implementation}
\subsection{Getting the data}
The database is of significant size, spanning 2075259 instances.
This is time consuming both for a human and for a cpu to work on.
Therefore, the script \code{get\_dataset.sh} has been written.
It accepts a numeric parameter of how many instances to be downloaded.
If no parameter is passed, the whole datased is downloaded.

\subsection{Reading the data}
The data reading step includes several subtle transformations on the data.
\para
Firstly, \code{Date} and \code{Time} columns are merged into a single \code{time} attribute.
This immensely aids operations with times, like verification or time differences.
\para
Then the rest of the attributes are converted to standard SI units.
In order to insure physically sound analysis, the measurement units of the attributes are included in their names.
\para
Power factor `pf` and Active energy in the rest of the house \code{active4.W} attributes are injected, as those will be often used.
\para
The whole thing is represented as a \code{tidyverse::tibble} - a modern best practice among R programmers. 

\subsection{Cleaning the data}
There are a couple major challanges in cleaning the data.
\para
One of them are missing values.
I have selected a conservative approach of dropping datapoints with missing values.
This decision has been motivated by several observations:
\begin{itemize}
    \item{The missing values are less than 2\% of the data.}
    \item{When values are missing, everything except \code{Date} and \code{Time} is missing at the same time.}
    \item{Although the new irregular timescale hinders the analysis (and particularly continuous time estimates such as regressions), this approach is believed to introduce the least ammount of subjectivity.}
\end{itemize}
\para
Another major challange were inconsistent values.
For example, very early in the analysis it was observed that Power factor values routinely exceed their theoretical maximum value of 1.0.
Power factor violations consitute about 52\% of the remaining data after dropping empty measurements.
The followind approaches were considered - and overjected - for coping with said problem.
\begin{itemize}
    \item{Drop offending roles - rejected because of high percentage of offending rows and low error band}
    \item{Correct \code{pf} and immediately related quantities - not feasable because of the large subjective skew to be introduced to the database.}
    \item{Clip \code{pf} to valid range - not performed as then the other qunatities would have produced ghost deviations and undetectable errors.}
\end{itemize}
Consequently, \code{pf} verification was just turned off.
All other violations were filtered out.


\subsection{Regression}
% ========================================================
\section{Conclusion}
Conclude the whole project in short text.

% ========================================================

 
\end{document}
