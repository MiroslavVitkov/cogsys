Hey guys, for the new assignment we should use the `evalb` program. It’s a C program and it’s needed to be compiled to work. To save this technical part, I’ve made a basic `evalb` web-app that you can upload the files directly, and you don’t need to compile it or run it locally:
https://evalb.herokuapp.com/ (edited)
Saturday, December 9th
Mahmud 12:33 AM
@jensinator I am having problem with calculating the labeled precision and unlabeled precesion

from the example given by lecture 8
I describe the data in tree
--Parse tree
(s
    (PP                        0
        (IN But)
            1
        (NP
            (DT the)
            2
        )
    )
    (NP-SBJ
        (NN Concept)
            3
    )
    (VP
        (VBJ   is )
            4
        (ADJP  
            (JJ    workable)
            5
        )
    )
)
and now putting the brackets for the words and corresponding tags
S(0,5)
PP(0,2)
IN(0,1)
NP(1,2)
DT(1,2)
NP-SBJ(2,3)
NN(2,3)
VP(3,5)
VBJ(3,4)
ADJP(4,5)
JJ(4,5) (edited)
Mahmud 12:40 AM
and corresponding gold file
(S
    0
    (CC But )
    1
    (NP-SBJ
        (DT the )
        2
        (NN Concept)
        3
    )
    (VP
        (VBJ is)
        4
        (ADJP
            (JJ Workable )
            5
        )
    )
)
12:44
and bracket is
S(0,5)
CC(0,1)
NP-SBJ(1,3)
DT(1,2)
NN(2,3)
VP(3,5)
VBJ(3,4)
ADJP(4,5)
JJ(4,5)
12:45
now what is the labeled and unlabeled precision here and how to calculate it
12:46
i can get 7 exact matches from the parse tree and the gold here
so can assume 7/11 is the labeled precision
12:47
but how to get the unlabeled precesion which is 10/11
12:47
?
Mahmud 12:57 AM
Or even in broader question, what output from the question 1 is required
nfel 1:07 AM
For precision, you consider all the constituents of the parse tree and check if they're also present in the gold tree. So out of the 11 constituents of the parse tree, there's this one `PP(0,2)` where its indices `0,2` are not matching up with any of the gold constituents and it's a mismatch for both unlabeled (10/11) and labeled precision.
For labeled precision, you additionally have `IN(0,1)`, `NP(1,2)` and `NP-SBJ(2,3)` whose indices *are* present in the gold tree (namely through `CC(0,1)`, `DT(1,2)` and `NN(2,3)`), but those constituents' labels don't match up. That's why you got three more mismatches, 7/11.

Similarly, for recall - where you start off by looking at the gold constituents - you got the `NP-SBJ(1,3)` whose indices don't have a corresponding constituent in the parse tree (unlabeled: 8/9). In addition, the `CC(0,1)` *has* a index match with `IN(0,1)`, but obviously their tags are different, so 7/9 for labeled recall.

It's late, so please correct me tomorrow morning, fellow experts! :wink:
Mahmud 1:14 AM
I see
nfel 1:22 AM
Afaik the result for question 1 should be the precision, recall and F1 scores for the whole corpus. Not sure what to take away from the Moodle discussion, but I guess macro-averaging is the way to go. http://rushdishams.blogspot.de/2011/08/micro-and-macro-average-of-precision.html This link is helpful for understanding the difference between micro and macro.
rushdishams.blogspot.de
Micro- and Macro-average of Precision, Recall and F-Score
I posted several articles explaining how precision and recall can be calculated, where F-Score is the equally weighted harmonic mean of them...
kuan 2:01 PM
it's helpful to test your scores against the examples in EVALB under the `sample` folder.  the test cases include many edge cases, for example when a test sentence does not match the gold sentence, or when a unary recursive derivation is repeated and you get different constituents with the same span and label.  `sample.rsl` contains detailed scores for each test case, and the macro-averaged total scores. (edited)
Sunday, December 10th
Mahmud 11:43 PM
@jensinator Is the data in Gold and parse file in CNF format?
jensinator 11:52 PM
no
Monday, December 11th
Ignatia 11:17 PM
Hi Nils,
Kuan Yu
it's helpful to test your scores against the examples in EVALB under the `sample` folder.  the test cases include many edge cases, for example when a test sentence does not match the gold sentence, or when a unary recursive derivation is repeated and you get different constituents with the same span and label.  `sample.rsl` contains detailed scores for each test case, and the macro-averaged total scores.
Posted in #advancednlpDec 9th at 2:01 PM
11:18
Oops - just ignore my msg :slightly_smiling_face:
Tuesday, December 12th
Mahmud 8:24 AM
@jensinator In the output generated in the evalb provided, Recall , precision and Tag accuracy. Are these Labeled or unlabeled refered to lecture slide?

And additionally is the tag accuracy is F1 score or not? also it would be helpful if you can give us reference or mention what other fields denotes for in the output
Jan 11:19 AM
How do i proceed in the 2nd task of the assingment?
I downloaded the parser and i can play arround witht the GUI, but how do i get the whole thing to work in python?
I also downloaded the extension package for python which is a tar.gz file, but i have no idea what do with it and i found no documentation.
Any help or pointers in the right direction is appreciated :smile:
2 replies
Last reply 1 day ago View thread
jensinator 11:42 AM
@Mahmud whether it's labeled or unlabeled depends on the parameter file. you are supposed to implement both. not sure what tag accuracy refers to, I suppose it's % correct pre-terminal nodes. you don't need this in your implementation
11:44
@Jan good question actually, I guess it's not always easy to turn these assignments into notebooks... I'd say you only need the parser to generate a txt file with trees (that you can then load into the notebook) so you could just submit that along with your notebook? I'm gonna post about this on moodle to make it more "official"
kuan 1:15 PM
running it from gui requires lots of postprocessing, and i'm not sure whether it's even possible to tell it to use the gold-standard segmentation, because `edu.stanford.nlp.parser.ui.Parser` doesn't seem to take the relevant options.  running it from command line requires knowledge about j*** :disappointed:
Mahmud 8:49 PM
There is an extensive manual including a FAQ available through the official page, and the parser is highly customizable. Fortunately, it also comes with predefined shell scripts illustrating its usage. Give them a try:

$ ./lexparser.sh data/testsent.txt
(ROOT (S (VP (VB Test) (NP (PRP me))) (. !)))

According to the notebook we can get a shell script from which we can generate with the test sentences. But unfortunately after searching a lot in the website given could not find the shell script. @jensinator can you please give the shell script which is mentioned in the notebook.
jensinator 9:15 PM
it's in the stanford parser directory
1 reply
6 days ago View thread
jensinator 9:39 PM
jensinator 9:40 PM
this is what the unzipped folder should look like
9:40
https://nlp.stanford.edu/software/stanford-parser-full-2017-06-09.zip
Mahmud 9:45 PM
Yes i just downloaded the english models
9:45
thanks
kuan 10:18 PM
seriously, has anyone found a button in stanford parser's gui for specifying options to use newlines as sentence boundaries and spaces as token boundaries?  otherwise the parser does its own segmentation and it'll be really hard to match the parse trees to the gold-standard.  i mean if you know java, you could just read the javadoc for `edu.stanford.nlp.parser.lexparser.LexicalizedParser` and find the options and everything is fine (until your jvm runs out of memory, that is).  but for people who don't know java at all ... i suppose lots of googling will have to do.  https://nlp.stanford.edu/nlp/javadoc/javanlp/ here's the javadoc btw. (edited)
jensinator 10:40 PM
I realize this thing is a pain to use, but I didn't wanna give the "solution" right away because I suppose struggling with this stuff is sort of the point...
10:43
I wouldn't bother with the GUI or any python integrations or whatever. just run it based off of lexparser.sh (linux/mac) or lexparser.bat. check the docs that kuan linked for `edu.stanford.nlp.parser.lexparser.LexicalizedParser`, that page has parameters you can pass to the parser towards the bottom
10:46
you can/should put those parameters right into `lexparser.sh/bat` (e.g. replace `-outputFormat`, and add other stuff that you think is needed/useful -- make sure to increase the memory to something like -mx2000m), and then you could run it like `lexparser.sh wsj.22.mrg.text > stanford_output.txt` to parse all sentences in the wsj file and put the trees into an output file (edited)
10:46
hope this helps someone... we can talk about this more on friday
10:47
also I disagree that you need to know java to use this thing, you just need general knowledge of how to run programs/read APIs and not be afraid of trying stuff out to see what happens
kuan 10:51 PM
well people who don't know java tend to run away when they see two thousand classes in an api.  it's just their survival instinct kicking in :slightly_smiling_face:
Wednesday, December 13th
Mahmud 12:39 AM
Parameters:
formatString - A comma separated list of ways to print each Tree. For instance, "penn" or "words,typedDependencies". Known formats are: oneline, penn, latexTree, xmlTree, words, wordsAndTags, rootSymbolOnly, dependencies, typedDependencies, typedDependenciesCollapsed, collocations, semanticGraph, conllStyleDependencies, conll2007. The last two are both tab-separated values formats. The latter has a lot more columns filled with underscores. All of them print a blank line after the output except for oneline. oneline is also not meaningful in XML output (it is ignored: use penn instead). (Use of typedDependenciesCollapsed is deprecated. It works but we recommend instead selecting a type of dependencies using the optionsString argument. Note in particular that typedDependenciesCollapsed does not do CC propagation, which we generally recommend.)
optionsString - Options that additionally specify how trees are to be printed (for instance, whether stemming should be done). Known options are: stem, lexicalize, markHeadNodes, xml, removeTopBracket, transChinese, includePunctuationDependencies, basicDependencies, treeDependencies, CCPropagatedDependencies, collapsedDependencies, nonCollapsedDependencies, nonCollapsedDependenciesSeparated, includeTags .
tlp - The TreebankLanguagePack used to do things like delete or ignore punctuation in output
hf - The HeadFinder used in printing output
12:40
try if anyone find proper option please share
12:41
Yes good news "oneline" works
12:42
Can anyone suggest how can we print the output to a file??.. please....@channel (edited)
Mahmud 12:53 AM
well got it
jensinator 9:43 PM
wtf is this group contract thing anyway :sweat_smile:
9:43
why did she make you do this?
Manja 9:43 PM
Weeeell...pretty good question ^^
burak 10:51 PM
It seems that I still cant grasp the assignment (actually only the first part yet) and that may be a really dumb question. Why do I need trees to compare? I thought I can just convert the parsed sentences to a format something like this

S(0,5)
CC(0,1)
NP-SBJ(1,3)
DT(1,2)
NN(2,3)
VP(3,5)
VBJ(3,4)
ADJP(4,5)
JJ(4,5)

and then compare two outcomes somehow after parsing them char by char or in other way. Am I missing something here? (edited)
jensinator 11:40 PM
not sure if I really understand that question... at the end of the day you don't need trees, true. you only need the constituents
11:41
I suppose you could parse the tree strings into constituents directly by keeping track of open/closed brackets
11:42
I'm only doing the assignment myself right now so I'm not sure what the best way to do this is, but I found nltk.Tree pretty helpful
11:43
BTW the assignment kind of lies, it's true that the trees *by default* won't keep track of indices but you can simply add this info yourself
11:44
nodes/leaves can be anything, not just strings. so e.g. you could replace the leaves by tuples (original_leave_string, index_in_sentence), which in turn allows you to easily grab the spans for all constituents
11:45
it really pays off to explore the Tree class a bit!
Thursday, December 14th
jensinator 12:59 PM
I also found ParentedTree helpful to remove traces. note that in a regular tree, the subtrees don't actually know that they're part of a bigger tree. in a ParentedTree they are aware of their parents. so you could, for example, go through the (pre-terminal) subtrees of a tree, check if they are -NONE- and if so, delete them from the tree
Paulito 2:02 PM
I thought only the tree is allowed and no special case of it^^
jensinator 2:21 PM
meh
jensinator 2:46 PM
in case that wasn't clear, I meant "feel free to use anything in nltk.tree"
Friday, December 15th
ali.hinnawe 12:40 PM
@jensinator do we have to compute 'Accuracy by constituent size', 'Accuracy by label' on whole line set or on individual lines?  (Problem 2) (edited)
jensinator 2:43 PM
Full dataset
2:44
I'll leave a moodle post clarifying how this stuff should be computed
jensinator 4:01 PM

@echo off

:: Runs the English PCFG parser on one or more files, printing trees only

:: usage: lexparser fileToparse

java -mx2000m -cp "*;" edu.stanford.nlp.parser.lexparser.LexicalizedParser -tokenized -sentences newline -outputFormat "oneline" edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz %1

also here's my .bat file for running the stanford parser. this is for windows. linux/max guys should use the .sh file so you will need to adjust it accordingly
jensinator 4:02 PM
then from your command line you can run lexparser.bat /path/to/file_with_sentences > /path/to/file_you_want_the_output_in
Saturday, December 16th
Vincent Ortland 9:36 PM
Should we provide the dependency structures in problem 3 as image files or is there a more sophisticated method I missed? :sweat_smile:
jensinator 10:31 PM
you could provide images, but there is a sort of standard format that looks something like this

```nmod:poss(dog-2, My-1)
nsubj(likes-4, dog-2)
advmod(likes-4, also-3)
root(ROOT-0, likes-4)
xcomp(likes-4, eating-5)
dobj(eating-5, sausage-6)
```

Vincent Ortland 11:05 PM
Ah thanks! That makes things more simple :)
Today
burak 12:19 AM
Should we be removing TOP or empty roots or, take them into consideration as well by replacing TOP in gold files with empty strings? (part 1)
jensinator 10:36 AM
coincidentally I just posted about that on moodle!
Tunç 12:35 PM
as a person having no experience working with java or terminal, this question might be silly for those who are experienced, but:
when i try to reach edu.stanford.nlp.parser.lexparser.LexicalizedParser, i receive an error: Could not find or load main class edu.stanford.nlp.parser.lexparser.LexicalizedParser
can someone tell me what i am doing wrong?
Sören 12:55 PM
@Tunç I had the same error yesterday. You have to run the .bat file from its folder and not from a parent folder. So your command might look like this: lexparser.bat ..\parsing-eval\wsj.22.mrg.text > ../output.txt
1 reply
Today at 1:25 PM View thread
freya 7:28 PM
i'm a little bit confused by the posts on moodle. for the first part, we're supposed to work out the stuff for the whole dataset - is that enough as output? or are we supposed to do both, i.e. output the values for each tree and then also the whole dataset?
jensinator 9:27 PM
well I guess as tatjana has the last word on this stuff -- both
9:27
although I would prefer not to have 1700 lines of output in the notebooks...
new messages
Ignatia 9:33 PM
how about printing only the mean accuracies on the NB, and attaching a file with the detail results ?


---
def dicts(t): return {k: dicts(t[k]) for k in t}

Now we can prettyprint the structure with pprint(dicts(taxonomy)):

add(taxonomy,
    'Animalia>Chordata>Mammalia>Cetacea>Balaenopteridae>Balaenoptera>blue whale'.split('>'))

We can implement this simply as:

def add(t, path):
  for node in path:
    t = t[node]
