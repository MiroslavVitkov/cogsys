\documentclass{article}


\usepackage{mathtools}


\newcommand{\para}[0]{\par\vspace{0.2cm}\noindent}
\newcommand{\define}[2]{\textbf{#1} - {#2}.  \para}


\begin{document}
\section{Models, Data, Learning Problems}
\subsection{Taxonomy of learning problems}
\define{random variable}
           {a (vector) variable whose possible values are outcomes of a random phenomenon}
\define{instance}
           {a single outcome}
\define{model}
           {a prediction mechanism; a function that maps an instance to a value of the target variable}
\define{linear regression model}
           {$y_\theta(x) = x^T \theta + \theta_0$}
\define{unsupervised learning}
           {find structure in data}
\define{supervised learning}
           {find the function most likely to have generated the training data}
\define{reinforcement learning}
           {control a dynamic system; the model performs experiments(exploration) while keeping the system operating nominally(exploitation)}


\subsection{Models}
\define{loss}
           {$l(y_\theta(x_i), y_i)$ - measures agreement between model predictions and true labels}
\define{empirical risk}
           {$\hat R(\theta) = 1/n \sum_{i=1}^n l(y_\theta(x_i), y_i)$ - expected loss over all training data $T_n$}
\define{zero-one loss}
           {$l_{0/1}$ - can be parameterised for false positive and false negative costs}
$$l_{c_FP, c_FN}(x_i) =
  \begin{cases}
    0         & \quad \mathrm{if } f(x_i) = y_i  \\
    c_{FP}    & \quad \mathrm{if } x_i = +1, y_i = -1  \\
    c_{FN}    & \quad \mathrm{if } x_i = -1, y_i = +1  \\
  \end{cases}$$


\subsection{Classification}
\define{searching for a model}
           {$\theta^* = {argmin}_\theta \sum_{i=1}^n \hat R$ - unregularized, this can have 0, 1 or many solutions}

\define{regularizer}
           {expresses prior knowledge, does not depend on the data  \\
            $l0$ - count of nonzero parameters  \\
            $l1$ - sum  \\
            $l2$ - sum of squares}

\define{regularized empirical risk}
           {$\hat R(\theta) = \frac{1}{n} \sum_{i=1}^n l(y_\theta(x_i), y_i) + \lambda \Omega (\theta)$}

Justification for using regularizers:
\begin{itemize}
    \item{regularized loss model equivalent to MAP model in Bayesian statistics}
    \item{lower theoretical upper bond on OOB error}
    \item{select among many equally good solutions to the loss}
    \item{avoid model sensitivity to training data (stability)}
\end{itemize}

\define{expected OOB loss}{
    $$R(\theta) = \sum_y \int l(y_\theta(x), y) p(x, y) dx$$
    where $p(x, y)$ - true generating joint probability distribution}

A trivial way to obtain full in-band accuracy - remember all training instances in a table.  \\
\par
Model evaluation methods:
\begin{itemize}
    \item{training and test datasets}
    \item{N-fold cross validation.}
\end{itemize}


\end{document}
