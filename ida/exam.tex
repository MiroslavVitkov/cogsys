\documentclass{article}


\usepackage{enumitem}
\usepackage{mathtools}


% Global '\begin{itemize}[noitemsep,topsep=0pt,parsep=0pt,partopsep=0pt]'
\setitemize{noitemsep,topsep=0pt,parsep=0pt,partopsep=0pt}


\newcommand{\para}[0]{\par\vspace{0.2cm}\noindent}
\newcommand{\define}[2]{\textbf{#1} - {#2}.  \para}


\begin{document}
\section{Stats refresher}
\define{random variable}
           {a (vector) variable whose possible values are outcomes of a random phenomenon}
\define{joint distribution}
           {n-dimensional probability distribution over n variables}
\define{marginal distribution}
           {a joint distribution over a subset of the variables; the ignored variables are summed over}
\define{information entropy}
           {the average rate at which information is produced by a stochastic source of data}
$$ H(X) = - \sum p(x_i) \log p(x_i) \text{,  [bits, nats, bans]}$$
\para
In the case of $p(x_i) = 0$ for some $i$, the value of the corresponding summand $0 \log(0)$ is taken to be 0, which is consistent with the limit:
$$ \lim_{p \to 0+} p \log(p) = 0$$

\section{Models, Data, Learning Problems}
\subsection{Taxonomy of learning problems}
\define{instance}
           {a random variable value; a specific outcome}
\define{model}
           {a prediction mechanism; a function that maps an instance to a value of the target variable}
\define{linear regression model}
           {$y_\theta(x) = x^T \theta + \theta_0$}
\textbf{unsupervised learning}
           \begin{itemize}
                \item{find structure in data}
                \item{find attributes which describe the data well}
                \item{anomaly detection}
           \end{itemize}
\para
\define{supervised learning}
           {find the function most likely to have generated the training data}
\define{reinforcement learning}
           {control a dynamic system; the model performs experiments(exploration) while keeping the system operating nominally(exploitation)}


\subsection{Models}
\define{loss}
           {$l(y_\theta(x_i), y_i)$ - measures agreement between model predictions and true labels}
\define{empirical risk}
           {expected loss under an assumed distribution}
\define{empirical risk}
           {$\hat R(\theta) = 1/n \sum_{i=1}^n l(y_\theta(x_i), y_i)$ - expected loss over all training data $T_n$}
\define{zero-one loss}
           {$l_{0/1}$ - can be parameterised for false positive and false negative costs}
$$l_{c_FP, c_FN}(x_i) =
  \begin{cases}
    0         & \quad \mathrm{if } f(x_i) = y_i  \\
    c_{FP}    & \quad \mathrm{if } x_i = +1, y_i = -1  \\
    c_{FN}    & \quad \mathrm{if } x_i = -1, y_i = +1  \\
  \end{cases}$$


\subsection{Classification}
\define{searching for a model}
           {$\theta^* = {argmin}_\theta \sum_{i=1}^n \hat R$ - unregularized, this can have 0, 1 or many solutions}

\define{regularizer}
           {expresses prior knowledge, does not depend on the data  \\
            $l0$ - count of nonzero parameters, rarely used  \\
            $l1$ - favours sparse models  \\
            $l2$ - prevents any single parameter from becoming too important}
\para
\define{regularized empirical risk}
           {$\hat R(\theta) = \frac{1}{n} \sum_{i=1}^n l(y_\theta(x_i), y_i) + \lambda \Omega (\theta)$}

Justification for using regularizers:
\begin{itemize}
    \item{regularized loss model equivalent to MAP model in Bayesian statistics}
    \item{lower theoretical upper bond on OOB error}
    \item{select among many equally good solutions to the loss}
    \item{avoid model sensitivity to training data (stability)}
\end{itemize}
\para
\define{expected OOB loss}{
    $$R(\theta) = \sum_y \int l(y_\theta(x), y) p(x, y) dx$$
    where $p(x, y)$ - true generating joint probability distribution}

A trivial way to obtain full in-band accuracy - remember all training instances in a table.  \\
\par
Model evaluation methods:
\begin{itemize}
    \item{training and test datasets}
    \item{N-fold cross validation.}
\end{itemize}


\section{Problem Analysis and Data Prepossessing}
\subsection{types of problems}
Data availability challenges
\begin{itemize}
    \item{number of data: too few? too many for a single CPU main memory?}
    \item{number of attributes: too few? too many? sparse?}
    \item{quality: missing values? erroneous values? measurement error?}
\end{itemize}


Data integration challenges
\begin{itemize}
    \item{format and measurement units}
    \item{identify same/similar attributes in the different databases}
    \item{conflicts}
    \item{redundancy}
\end{itemize}


\subsection{Representation properties of data}
\begin{itemize}
    \item{number of classes - 2 vs 50 000}
    \item{class ratio - balanced vs rare classes}
    \item{$p_{train}(x) = p_{application}(x)$ - same generating process}
    \item{$p_{train}(y|x) = p_{application}(y|x)$ - same labelling process}
    \item{is the training sample representative of class frequency? else we are operating under 'covariate shift'}
    \item{data getting old}
    \item{separate data sources with different quality measures}
\end{itemize}
\para
\define{data warehouse}
           {a database which has been optimised for analytic processes, instead of for transactional processes}


\subsection{Encoding}
Linear models expect numeric inputs.
\para
Encode categorical variables one-hot, otherwise we are making an assumption of orderliness of the values.
\para
\define{TF representation}
           {term frequency - a set of \{word:count\},
            possibly trimmed below e.g. f=3}
\define{TFIDF representation}
           {terms that occur often (or, and, is) carry little semantic power - assign lower weights}
            $$\text{inverse document frequency}
                = \log \frac{\text{num documents}}{\text{num documents that contain the term}}
                = \log \frac{N}{n_T}$$

$$
TFIDF(x) = \frac{1}{|x|}
 \begin{pmatrix}
  TF({term}_1) * IDF({term}_1)  \\
  \vdots  \\
  TF({term}_n) * IDF({term}_n)  \\
 \end{pmatrix}
$$

where $|d|$ - count of words in this text of the corpus
\para
To preserve structure information in text, n-grams are used.
We have to introduce 1 new attribute per n-tuple.
Fortunately results are sparse.


\subsection{Feature normalisation}
Non-normalised data results in unbound weights.
The regularizer doesn't know if a weight is large because the feature is important, or because of scaling.
\begin{itemize}
    \item{min/max}
    \item{z-score}
    \item{decimal scaling}
    \item{log scaling}
\end{itemize}


\subsection{Introducing new features}
Hand-crafting new features as function of old features, which is outside the model's search space.
E.g. polynomial kernels consider all polynomials of the input variables, but linear models don't.


\subsection{Handling missing, erroneous values}
\begin{itemize}
    \item{delete instances (if few) or attributes(if almost always missing)}
    \item{add a value 'missing' OR introduce a binary attribute val\_missing}
    \item{interpolate - class specific mean/median; or infer most likely value}
\end{itemize}
\para
Erroneous values identification is performed by either binning or clustering.
Small resulting sets are suspect outliers.
\par
Some classes might exhibit missing values more often then others e.g. user provided credit rating.


\subsection{Feature selection}
\begin{itemize}
    \item{PCA}
    \item{foreword/backward elimination - try model with 1 attribute, test, train with 2 attributes, test, if better continue}
    \item{for a linear model - train a model on all attributes, delete attributes with smallest weights, retrain}
\end{itemize}


\section{Decision trees}
\define{test node}
           {tests one attribute}
\define{edge}
           {one specific outcome of the testing}
\define{terminal node}
           {contains a prediction}

Advantages:
    understandable to a domain expert
    fast predictions
    nonlinear model - thus can represent nonlinear functions

\end{document}
