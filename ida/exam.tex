\documentclass{article}


\usepackage{mathtools}


\newcommand{\para}[0]{\par\vspace{0.2cm}\noindent}
\newcommand{\define}[2]{\textbf{#1} - {#2}.  \para}


\begin{document}
\section{Models, Data, Learning Problems}
\subsection{Taxonomy of learning problems}
\define{random variable}
           {a (vector) variable whose possible values are outcomes of a random phenomenon}
\define{instance}
           {a single outcome}
\define{model}
           {a prediction mechanism; a function that maps an instance to a value of the target variable}
\define{linear regression model}
           {$y_\theta(x) = x^T \theta + \theta_0$}
\define{unsupervised learning}
           {find structure in data}
\define{supervised learning}
           {find the function most likely to have generated the training data}
\define{reinforcement learning}
           {control a dynamic system; the model performs experiments(exploration) while keeping the system operating nominally(exploitation)}


\subsection{Models}
\define{loss}
           {$l(y_\theta(x_i), y_i)$ - measures agreement between model predictions and true labels}
\define{risk}
           {$\hat R(\theta) = 1/n \sum_{i=1}^n l(y_\theta(x_i), y_i)$ - expected loss over all training data $T_n$}
\define{zero-one loss}
           {$l_{0/1}$ - can be parametrised for false positive and false negative costs}
$$l_{c_FP, c_FN}(x_i) =
  \begin{cases}
    0         & \quad \mathrm{if } f(x_i) = y_i  \\
    c_{FP}    & \quad \mathrm{if } x_i = +1, y_i = -1  \\
    c_{FN}    & \quad \mathrm{if } x_i = -1, y_i = +1  \\
  \end{cases}$$


\subsection{Classification}
%Regression - l2


%Searching for a model - $\theta*$ which minimises $R\hat$

%Learning is an _optimization_ problem, because it can produce zero, one or many best solutions.

\define{regularizer}{expresses prior knowledge, does not depend on the data.  \\
    $l0$ - count of nonzero parameters  \\
    $l1$ - sum  \\
    $l2$ - sum of squares}

%optimization criterion: $R^\hat(\theta) = 1/n \sum_{i=1}^n l(y_\theta(x_i), y_i) + \lambda \sigma (\theta)$

%Justification for using regularizers:
%    lower theoretical  upper bond on OOB error
%    select among many equally good solutions to the loss
%    avoid model sensitivity to training data (stability)

\define{OOB performance}{
    $$R(\theta) = \sum_y \int l(y_\theta(x), y) p(x, y) dx$$
    where $p(x, y)$ - true generating joint probability distribution}
    

\end{document}
