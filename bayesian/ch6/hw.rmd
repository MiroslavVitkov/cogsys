#!/usr/bin/env Rscript


#<!---
library(rmarkdown)
argv <- commandArgs(trailingOnly=FALSE)
fname <- sub("--file=", "", argv[grep("--file=", argv)])
render(fname, output_format="pdf_document")
quit(status=0)
#-->


---
title: "Overfitting, Regularization, and Information Criteria"
author: Miroslav Vitkov
date: "compiled on: `r Sys.time()`"
---


6E1. State the three motivating criteria that define information entropy.
Try to express each in your own words.  
A:

  - continuous - no surprises
  - monotonously increasing with increasing outcome possibilities
  - additive on different variables

Note: given the author's liberal wording,
besides $H(p) = \sum p_i log(p_i)$,
there are many functions satisfying all three criterions.
For example:
$f(p_1, p_2, ..., p_n) = count(p_i)$  

  - continuous: it's constant
  - monotonous: yup
  - additive: let k be cardinality of first set of events, l - cardinality of second set; $f(k) + f(l) = k(k+l) = k+l$

where is the problem? - continuity at $p_i$ -> 0


6E2. Suppose a coin is weighted such that, when it is tossed and lands on a table, it comes up heads 70% of the time.
What is the entropy of this coin?  
A:
```{r coin}
H <- function(p)
{
    p <- p[ p>0 ]  # lim x->0 (x log(x)) == 0
    -sum( p*log(p) )
}
H(c(.7, 1-.7))
```


6E3. Suppose a four-sided die is loaded such that, when tossed onto a table, it shows “1” 20%, “2” 25%, ”3” 25%, and ”4” 30% of the time.
What is the entropy of this die?  
A:
```{r die1}
H(c(.2, .25, .25, .3))
```


6E4. Suppose another four-sided die is loaded such that it never shows “4”.
The other three sides show equally often.
What is the entropy of this die?  
A:
```{r die2}
H(c(1/3, 1/3, 1/3, 0))
```


6M1. Write down and compare the definitions of AIC, DIC, and WAIC.
Which of these criteria is most general?
Which assumptions are required to transform a more general criterion into a less general one?  
A:  
${AIC} = D_{train} + 2p$, where  
$D_{train} = -2 \sum log(q_i)$ - in sample variance  
$p$ - number of parameters to be estimated by the model
\par
${DIC} = 2 \bar{D} + \hat{D}$, where  
$\bar{D} = \sum p_i D_i$ - expectation for D  
$\hat{D} = D(\bar{\theta})$ - deviance at expected values of model parameters
\par
$WAIC = -2({lppd} - p_{WAIC})$  
${lppd} = \sum {log}(Pr(y_i))$ - log pointwise predictive density  
$Pr(y_i) - average (over parameter values) likelyhood of $i$-th datapoint$  
$p_{WAIC} = \sum {Var}(y_i)$ - effective number of parameters  
\par
WAIC + multivariate Gaussian posterior + sample size much larger than number or parameters = DIC  
DIC + flat priors = AIC  
\par


6M2. Explain the difference between model selection and model averaging.
What information is lost under model selection?
What information is lost under model averaging?  
A:  
In the former we select the model with lowest deviance.
In the latter we use the distribution of deviance to compose model weights, which in turn help produce an ensemble prediction.
\par
Under model selection, the information how much was the winner better than all the rest is lost.
Under model averaging, the prediction of every single model is lost.


6M3. When comparing models with an information criterion, why must all models be fit to exactly the same observations?
What would happen to the information criterion values, if the models were fit to different numbers of observations?
Perform some experiments, if you are not sure.  
A:  
For models fitted over more data, the deviance will be better, despite worse out of sample performance.


6M4. What happens to the effective number of parameters, as measured by DIC or WAIC, as a prior becomes more concentrated?
Why?
Perform some experiments, if you are not sure.  
A:
It decreases.


6M5. Provide an informal explanation of why informative priors reduce overfitting.  
A:  
Just like adding a term to a cost function, adding a non-flat prior makes the model dislike parameter values, which are "worse" than a flat prior.


6M6. Provide an information explanation of why overly informative priors result in underfitting.  
A:  
They do not, if the priror is spot on.
Else, we are multiplaying observed data by prior likelyhood close to zero, resulting in close to zero values.


6H0  
```{r setup_hard0}
d <- read.csv("res/Howell1.csv", header=TRUE, sep=';')

# Nope, no duck typing.
# d <- as_tibble(d)

d$age <- (d$age - mean(d$age))/sd(d$age)
set.seed( 1000 )
i <- sample(1:nrow(d),size=nrow(d)/2)
d1 <- d[ i , ]
d2 <- d[ -i , ]

```{r setup_hard1, results='hide', message=FALSE}
# Would have certaily been nice if the retrinking package wasn't a bowl of spagetti.
#source1 <- function(path, fun)
#{
#  source(path, local= TRUE)
#  get(fun)
#}
#max.a.posteriori <- source1("res/map.r","map")
library(rethinking)
```
```{r setup_hard2}
poly.mu <- function( order=1 )
{
    mu <- "mu <- a"
    if( order > 0 )
    {
        for( i in 1:order )
        {
            mu <- paste( mu, " + b", i, " * weight^", i, sep="" )
        }
    }
    mu
}


poly.fit <- function( order=1 )
{
    # I am sure this is quite messy, but don't speak enough R to
    # conform to best practices of efficiency and correctness.
    first <- "rethinking::map( data=d1, flist=alist( height ~ dnorm( mu, sigma )"
    first <- paste( first, ", sigma ~ dunif( 0, 50 ), a ~ dnorm( 178, 100 ), " )
    third <- ""
    fourth <- ", start=list( "
    if( order > 0 )
    {
        for( i in 1:order )
        {
            third <- paste( third, ", b", i, " ~ dnorm( 0, 1 )", sep="")
            if( i > 1)
            {
                fourth <- paste(fourth, ", ", sep="")
            }
            fourth <- paste( fourth, "b", i, " = 0 ", sep="" )
        }
    }
    mu <- poly.mu(order)
    mu
    expr <- paste( first, mu, third, " )", fourth, " ) )", sep="" )
    eval( parse( text=expr ) )
}


# Note that we are not filtering away children!
(m1 <- poly.fit(1))
(m2 <- poly.fit(2))
(m3 <- poly.fit(3))
(m4 <- poly.fit(4))
# Nope, this one refuses to fit.
# I also tried b_i ~ dnorm(0, x / i) or i squared to no avail.
(m5 <- poly.fit(5))
(m6 <- poly.fit(6))


# Read individual parameters off the trained models.
get.param <- function( model, param.name )
{
    c <- model@coef[ param.name ]
    as.numeric( c )
}
get.param(m1, 'sigma')
```


6H1. Compare the models above, using WAIC.
Compare the model rankings, as well as the WAIC weights.  
A:  
```{r waic_compare}
rethinking::compare( m1 , m2, m3, m4 )
```


6H2. For each model, produce a plot with model averaged mean and 97% confidence interval of the mean, superimposed on the raw data.
How do predictions differ across models?  
A:  
```{r plot_models}
# I love this language more and more.
# The '+' cannot be on the new line.
# Thus, every time when adding or removing a geometry,
# 2 lines need to be edited. Go figure.

# todo: obtain model prediction by poly.mu(i)

ggplot() +
    geom_point( data=d1, mapping=aes(x=1:nrow(d1), y=height) )
  #  + geom_abline(intercept=4.45731476, slope=0.02295616)
# TODO: confidence intervals in R?
# TODO: or even better: stats/miniexam2.R
```


6H3. Now also plot the model averaged predictions, across all models.
In what ways do the averaged predictions differ from the predictions of the model with the lowest WAIC value?  
A:  
```{r ensemble, results='hide', message=FALSE}
avv <- rethinking::ensemble( m1, m2, m3, m4 )
```
```{r}
avv[0][0]
#ggplot(data=d1, mapping=aes(x=1:nrow(d1), y=height)) +
#    geom_point()
```


6H4. Compute the test-sample deviance for each model.
This means calculating deviance, but using the data in d2 now.
You can compute the log-likelihood of the height data with:
```{r example_dev,eval = FALSE}
sum( dnorm( d2$height , mu , sigma , log=TRUE ) )
```
where mu is a vector of predicted means (based upon age values and MAP parameters) and sigma is the MAP standard deviation.  
A:
```{r}
```


6H5. Compare the deviances from 6H4 to the WAIC values.
It might be easier to compare if you subtract the smallest value in each list from the others.
For example, subtract the minimum WAIC from all of the WAIC values so that the best WAIC is normalized to zero.
Which model makes the best out-of-sample predictions in this case?
Does WAIC do a good job of estimating the test deviance?  
A:
```{r}
```


6H6. Consider the following model:
$$h_i \sim Normal(\mu_i , \sigma)$$
$$\mu_i = \alpha + \beta_1 x_i + \beta_2 x_i^2 + \beta_3 x_i^3 + \beta_4 x_i^4 + \beta_5 x_i^5 + \beta_6 x_i^6$$
$$\beta_1 \sim Normal(0, 5)$$
$$\beta_2 \sim Normal(0, 5)$$
$$\beta_3 \sim Normal(0, 5)$$
$$\beta_4 \sim Normal(0, 5)$$
$$\beta_5 \sim Normal(0, 5)$$
$$\beta_6 \sim Normal(0, 5)$$
and assume flat (or nearly flat) priors on $\alpha$ and $\sigma$.
This model contains more strongly regularizing priors on the coefficients.
First, fit this model to the data in d1.
Report the MAP estimates and plot the implied predictions.
Then compute the out-of-sample deviance using the data in d2, using MAP estimates from the model fit to d1 only.
How does this model, using regularizing priors, compare to the best WAIC model from earlier?
How do you interpret this result?  
A:
```{r}
```
